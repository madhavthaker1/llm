{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "a3f04bcf-f76c-4172-82ea-76aa683a1305",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5c35f445-7e40-4f56-8a1b-be004736abc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftModel\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_transformers import Html2TextTransformer\n",
    "from langchain.document_loaders import AsyncChromiumLoader\n",
    "\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "10517235-0939-474a-9825-0381a29decce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftModel\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_transformers import Html2TextTransformer\n",
    "from langchain.document_loaders import AsyncChromiumLoader\n",
    "\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "607a21d4-c7d9-47d8-aed1-79eac725f4c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Your GPU supports bfloat16: accelerate training with bf16=True\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08b59e26b1ba4cb58ce2196d95406b14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#################################################################\n",
    "# Tokenizer\n",
    "#################################################################\n",
    "\n",
    "model_name='mistralai/Mistral-7B-Instruct-v0.1'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "#################################################################\n",
    "# bitsandbytes parameters\n",
    "#################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "#################################################################\n",
    "# Set up quantization config\n",
    "#################################################################\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "#################################################################\n",
    "# Load pre-trained config\n",
    "#################################################################\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "f9876d71-4271-4d41-b259-e15ce74da071",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_generation_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.01,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=1000,\n",
    ")\n",
    "\n",
    "mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "30c843ff-ae39-4beb-82c6-e500d0961f5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 4148, which is longer than the specified 300\n",
      "Created a chunk of size 500, which is longer than the specified 300\n",
      "Created a chunk of size 365, which is longer than the specified 300\n",
      "Created a chunk of size 312, which is longer than the specified 300\n",
      "Created a chunk of size 515, which is longer than the specified 300\n",
      "Created a chunk of size 584, which is longer than the specified 300\n",
      "Created a chunk of size 1119, which is longer than the specified 300\n",
      "Created a chunk of size 4148, which is longer than the specified 300\n",
      "Created a chunk of size 567, which is longer than the specified 300\n",
      "Created a chunk of size 316, which is longer than the specified 300\n",
      "Created a chunk of size 449, which is longer than the specified 300\n",
      "Created a chunk of size 479, which is longer than the specified 300\n",
      "Created a chunk of size 533, which is longer than the specified 300\n",
      "Created a chunk of size 446, which is longer than the specified 300\n",
      "Created a chunk of size 317, which is longer than the specified 300\n",
      "Created a chunk of size 4148, which is longer than the specified 300\n",
      "Created a chunk of size 450, which is longer than the specified 300\n",
      "Created a chunk of size 462, which is longer than the specified 300\n",
      "Created a chunk of size 469, which is longer than the specified 300\n",
      "Created a chunk of size 432, which is longer than the specified 300\n",
      "Created a chunk of size 380, which is longer than the specified 300\n",
      "Created a chunk of size 4148, which is longer than the specified 300\n",
      "Created a chunk of size 422, which is longer than the specified 300\n",
      "Created a chunk of size 498, which is longer than the specified 300\n",
      "Created a chunk of size 413, which is longer than the specified 300\n",
      "Created a chunk of size 456, which is longer than the specified 300\n",
      "Created a chunk of size 404, which is longer than the specified 300\n",
      "Created a chunk of size 380, which is longer than the specified 300\n",
      "Created a chunk of size 346, which is longer than the specified 300\n",
      "Created a chunk of size 4148, which is longer than the specified 300\n",
      "Created a chunk of size 403, which is longer than the specified 300\n",
      "Created a chunk of size 541, which is longer than the specified 300\n",
      "Created a chunk of size 324, which is longer than the specified 300\n",
      "Created a chunk of size 305, which is longer than the specified 300\n",
      "Created a chunk of size 333, which is longer than the specified 300\n",
      "Created a chunk of size 457, which is longer than the specified 300\n",
      "Created a chunk of size 421, which is longer than the specified 300\n",
      "Created a chunk of size 528, which is longer than the specified 300\n",
      "Created a chunk of size 592, which is longer than the specified 300\n",
      "Created a chunk of size 633, which is longer than the specified 300\n",
      "Created a chunk of size 303, which is longer than the specified 300\n",
      "Created a chunk of size 658, which is longer than the specified 300\n",
      "Created a chunk of size 520, which is longer than the specified 300\n",
      "Created a chunk of size 438, which is longer than the specified 300\n",
      "Created a chunk of size 333, which is longer than the specified 300\n",
      "Created a chunk of size 581, which is longer than the specified 300\n",
      "Created a chunk of size 432, which is longer than the specified 300\n",
      "Created a chunk of size 574, which is longer than the specified 300\n",
      "Created a chunk of size 741, which is longer than the specified 300\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Articles to index\n",
    "articles = [\"https://www.fantasypros.com/2023/11/rival-fantasy-nfl-week-10/\",\n",
    "            \"https://www.fantasypros.com/2023/11/5-stats-to-know-before-setting-your-fantasy-lineup-week-10/\",\n",
    "            \"https://www.fantasypros.com/2023/11/nfl-week-10-sleeper-picks-player-predictions-2023/\",\n",
    "            \"https://www.fantasypros.com/2023/11/nfl-dfs-week-10-stacking-advice-picks-2023-fantasy-football/\",\n",
    "            \"https://www.fantasypros.com/2023/11/players-to-buy-low-sell-high-trade-advice-2023-fantasy-football/\"]\n",
    "\n",
    "# Scrapes the blogs above\n",
    "loader = AsyncChromiumLoader(articles)\n",
    "docs = loader.load()\n",
    "\n",
    "# Converts HTML to plain text \n",
    "html2text = Html2TextTransformer()\n",
    "docs_transformed = html2text.transform_documents(docs)\n",
    "\n",
    "# Chunk text\n",
    "text_splitter = CharacterTextSplitter(chunk_size=300, \n",
    "                                      chunk_overlap=0)\n",
    "chunked_documents = text_splitter.split_documents(docs_transformed)\n",
    "\n",
    "# Load chunked documents into the FAISS index\n",
    "db = FAISS.from_documents(chunked_documents, \n",
    "                          HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2'))\n",
    "\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "bc465cc0-8851-4171-9763-88d4571c70e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents.agent_toolkits import create_retriever_tool\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "class RetrieverInput(BaseModel):\n",
    "    query: str = Field(description=\"query to look up in retriever\")\n",
    "\n",
    "tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"football_search\",\n",
    "    \"Searches and returns documents regarding fantasy football.\",\n",
    ")\n",
    "tools = [tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "591b57b9-14da-4eda-ab40-872a30edd3ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, openai_api_key = \"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "91874fe7-bc0e-4be5-ade8-1bdbc2d2c837",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseLLM.predict_messages of HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7fd1515b5a20>)>"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistral_llm.predict_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "b2ede358-0a65-417a-b169-c700fcf06e22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Module implements an agent that uses OpenAI's APIs function enabled API.\"\"\"\n",
    "from typing import Any, List, Optional, Sequence, Tuple, Union\n",
    "\n",
    "from langchain_core.agents import AgentAction, AgentFinish\n",
    "from langchain_core.language_models import BaseLanguageModel\n",
    "from langchain_core.messages import (\n",
    "    BaseMessage,\n",
    "    SystemMessage,\n",
    ")\n",
    "from langchain_core.prompts import BasePromptTemplate\n",
    "from langchain_core.prompts.chat import (\n",
    "    BaseMessagePromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "from langchain_core.pydantic_v1 import root_validator\n",
    "from langchain_core.tools import BaseTool\n",
    "\n",
    "from langchain.agents import BaseSingleActionAgent\n",
    "from langchain.agents.format_scratchpad.openai_functions import (\n",
    "    format_to_openai_function_messages,\n",
    ")\n",
    "from langchain.agents.output_parsers.openai_functions import (\n",
    "    OpenAIFunctionsAgentOutputParser,\n",
    ")\n",
    "from langchain.callbacks.base import BaseCallbackManager\n",
    "from langchain.callbacks.manager import Callbacks\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "\n",
    "\n",
    "class OpenAIFunctionsAgentDev(BaseSingleActionAgent):\n",
    "    \"\"\"An Agent driven by OpenAIs function powered API.\n",
    "\n",
    "    Args:\n",
    "        llm: This should be an instance of ChatOpenAI, specifically a model\n",
    "            that supports using `functions`.\n",
    "        tools: The tools this agent has access to.\n",
    "        prompt: The prompt for this agent, should support agent_scratchpad as one\n",
    "            of the variables. For an easy way to construct this prompt, use\n",
    "            `OpenAIFunctionsAgent.create_prompt(...)`\n",
    "    \"\"\"\n",
    "\n",
    "    llm: BaseLanguageModel\n",
    "    tools: Sequence[BaseTool]\n",
    "    prompt: BasePromptTemplate\n",
    "\n",
    "    def get_allowed_tools(self) -> List[str]:\n",
    "        \"\"\"Get allowed tools.\"\"\"\n",
    "        return [t.name for t in self.tools]\n",
    "\n",
    "    @root_validator\n",
    "    def validate_prompt(cls, values: dict) -> dict:\n",
    "        prompt: BasePromptTemplate = values[\"prompt\"]\n",
    "        if \"agent_scratchpad\" not in prompt.input_variables:\n",
    "            raise ValueError(\n",
    "                \"`agent_scratchpad` should be one of the variables in the prompt, \"\n",
    "                f\"got {prompt.input_variables}\"\n",
    "            )\n",
    "        return values\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        \"\"\"Get input keys. Input refers to user input here.\"\"\"\n",
    "        return [\"input\"]\n",
    "\n",
    "    @property\n",
    "    def functions(self) -> List[dict]:\n",
    "        print(\"format tools\", [dict(format_tool_to_openai_function(t)) for t in self.tools])\n",
    "        return [dict(format_tool_to_openai_function(t)) for t in self.tools]\n",
    "\n",
    "    def plan(\n",
    "        self,\n",
    "        intermediate_steps: List[Tuple[AgentAction, str]],\n",
    "        callbacks: Callbacks = None,\n",
    "        with_functions: bool = True,\n",
    "        **kwargs: Any,\n",
    "    ) -> Union[AgentAction, AgentFinish]:\n",
    "        \"\"\"Given input, decided what to do.\n",
    "\n",
    "        Args:\n",
    "            intermediate_steps: Steps the LLM has taken to date, along with observations\n",
    "            **kwargs: User inputs.\n",
    "\n",
    "        Returns:\n",
    "            Action specifying what tool to use.\n",
    "        \"\"\"\n",
    "        print(\"intermediate_steps\", intermediate_steps)\n",
    "        \n",
    "        agent_scratchpad = format_to_openai_function_messages(intermediate_steps)\n",
    "        print(\"agent scratchpad\", agent_scratchpad)\n",
    "        selected_inputs = {\n",
    "            k: kwargs[k] for k in self.prompt.input_variables if k != \"agent_scratchpad\"\n",
    "        }\n",
    "        full_inputs = dict(**selected_inputs, agent_scratchpad=agent_scratchpad)\n",
    "        prompt = self.prompt.format_prompt(**full_inputs)\n",
    "        messages = prompt.to_messages()\n",
    "        print(\"with functions\", with_functions)\n",
    "        if with_functions:\n",
    "            \n",
    "            print(\"messages\", messages)\n",
    "            print(\"functions\", self.functions)\n",
    "            print(\"callbacks\", callbacks)\n",
    "            predicted_message = self.llm.predict_messages(\n",
    "                messages,\n",
    "                functions=self.functions,\n",
    "                callbacks=callbacks,\n",
    "            )\n",
    "\n",
    "            predicted_message = AIMessage(\n",
    "                content='',\n",
    "                additional_kwargs={\n",
    "                    'function_call': {\n",
    "                        'arguments': {\n",
    "                            'query': 'Gus Edwards'\n",
    "                        },\n",
    "                        'name': 'football_search'\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "            print(\"predicted_message\", type(predicted_message), predicted_message, )\n",
    "        else:\n",
    "            predicted_message = self.llm.predict_messages(\n",
    "                messages,\n",
    "                callbacks=callbacks,\n",
    "            )\n",
    "        agent_decision = OpenAIFunctionsAgentOutputParser._parse_ai_message(\n",
    "            predicted_message\n",
    "        )\n",
    "        \n",
    "        print(\"agent_decision\", agent_decision)\n",
    "        return agent_decision\n",
    "\n",
    "    async def aplan(\n",
    "        self,\n",
    "        intermediate_steps: List[Tuple[AgentAction, str]],\n",
    "        callbacks: Callbacks = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Union[AgentAction, AgentFinish]:\n",
    "        \"\"\"Given input, decided what to do.\n",
    "\n",
    "        Args:\n",
    "            intermediate_steps: Steps the LLM has taken to date,\n",
    "                along with observations\n",
    "            **kwargs: User inputs.\n",
    "\n",
    "        Returns:\n",
    "            Action specifying what tool to use.\n",
    "        \"\"\"\n",
    "        print()\n",
    "        print(\"intermediate steps\", intermediate_steps)\n",
    "        agent_scratchpad = format_to_openai_function_messages(intermediate_steps)\n",
    "        \n",
    "        print(\"agent scratchpad\", agent_scratchpad)\n",
    "        selected_inputs = {\n",
    "            k: kwargs[k] for k in self.prompt.input_variables if k != \"agent_scratchpad\"\n",
    "        }\n",
    "        full_inputs = dict(**selected_inputs, agent_scratchpad=agent_scratchpad)\n",
    "        prompt = self.prompt.format_prompt(**full_inputs)\n",
    "        messages = prompt.to_messages()\n",
    "        print(\"messages aplan\", messages)\n",
    "        predicted_message = await self.llm.apredict_messages(\n",
    "            messages, functions=self.functions, callbacks=callbacks\n",
    "        )\n",
    "        \n",
    "        # print(\"predicted_message\", predicted_message)\n",
    "        \n",
    "        agent_decision = OpenAIFunctionsAgentOutputParser._parse_ai_message(\n",
    "            predicted_message\n",
    "        )\n",
    "        \n",
    "        # print(\"agent_decision\",agent_decision)\n",
    "        return agent_decision\n",
    "\n",
    "    def return_stopped_response(\n",
    "        self,\n",
    "        early_stopping_method: str,\n",
    "        intermediate_steps: List[Tuple[AgentAction, str]],\n",
    "        **kwargs: Any,\n",
    "    ) -> AgentFinish:\n",
    "        \"\"\"Return response when agent has been stopped due to max iterations.\"\"\"\n",
    "        print()\n",
    "        print(\"intermediate steps return stopped\", intermediate_steps)\n",
    "        print(\"early stopping\", early_stopping_method)\n",
    "        print()\n",
    "        if early_stopping_method == \"force\":\n",
    "            # `force` just returns a constant string\n",
    "            return AgentFinish(\n",
    "                {\"output\": \"Agent stopped due to iteration limit or time limit.\"}, \"\"\n",
    "            )\n",
    "        elif early_stopping_method == \"generate\":\n",
    "            # Generate does one final forward pass\n",
    "            agent_decision = self.plan(\n",
    "                intermediate_steps, with_functions=False, **kwargs\n",
    "            )\n",
    "            \n",
    "            # print(\"agent decision 2\", agent_decision)\n",
    "            if isinstance(agent_decision, AgentFinish):\n",
    "                return agent_decision\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"got AgentAction with no functions provided: {agent_decision}\"\n",
    "                )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"early_stopping_method should be one of `force` or `generate`, \"\n",
    "                f\"got {early_stopping_method}\"\n",
    "            )\n",
    "\n",
    "    @classmethod\n",
    "    def create_prompt(\n",
    "        cls,\n",
    "        system_message: Optional[SystemMessage] = SystemMessage(\n",
    "            content=\"You are a helpful AI assistant.\"\n",
    "        ),\n",
    "        extra_prompt_messages: Optional[List[BaseMessagePromptTemplate]] = None,\n",
    "    ) -> BasePromptTemplate:\n",
    "        \"\"\"Create prompt for this agent.\n",
    "\n",
    "        Args:\n",
    "            system_message: Message to use as the system message that will be the\n",
    "                first in the prompt.\n",
    "            extra_prompt_messages: Prompt messages that will be placed between the\n",
    "                system message and the new human input.\n",
    "\n",
    "        Returns:\n",
    "            A prompt template to pass into this agent.\n",
    "        \"\"\"\n",
    "        _prompts = extra_prompt_messages or []\n",
    "        messages: List[Union[BaseMessagePromptTemplate, BaseMessage]]\n",
    "        if system_message:\n",
    "            messages = [system_message]\n",
    "        else:\n",
    "            messages = []\n",
    "\n",
    "        messages.extend(\n",
    "            [\n",
    "                *_prompts,\n",
    "                HumanMessagePromptTemplate.from_template(\"{input}\"),\n",
    "                MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "            ]\n",
    "        )\n",
    "        return ChatPromptTemplate(messages=messages)\n",
    "\n",
    "    @classmethod\n",
    "    def from_llm_and_tools(\n",
    "        cls,\n",
    "        llm: BaseLanguageModel,\n",
    "        tools: Sequence[BaseTool],\n",
    "        callback_manager: Optional[BaseCallbackManager] = None,\n",
    "        extra_prompt_messages: Optional[List[BaseMessagePromptTemplate]] = None,\n",
    "        system_message: Optional[SystemMessage] = SystemMessage(\n",
    "            content=\"You are a helpful AI assistant.\"\n",
    "        ),\n",
    "        **kwargs: Any,\n",
    "    ) -> BaseSingleActionAgent:\n",
    "        \"\"\"Construct an agent from an LLM and tools.\"\"\"\n",
    "        prompt = cls.create_prompt(\n",
    "            extra_prompt_messages=extra_prompt_messages,\n",
    "            system_message=system_message,\n",
    "        )\n",
    "        return cls(\n",
    "            llm=llm,\n",
    "            prompt=prompt,\n",
    "            tools=tools,\n",
    "            callback_manager=callback_manager,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "3c44fd94-03b6-4ad5-8135-15946e3d6a31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from agent_executor import AgentExecutorDev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "2e1857fe-5b08-4d60-860a-33ad1f44f1ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Any, List, Optional  # noqa: E501\n",
    "\n",
    "from langchain_core.language_models import BaseLanguageModel\n",
    "from langchain_core.memory import BaseMemory\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts.chat import MessagesPlaceholder\n",
    "\n",
    "from langchain.agents.agent import AgentExecutor\n",
    "from langchain.agents.openai_functions_agent.agent_token_buffer_memory import (\n",
    "    AgentTokenBufferMemory,\n",
    ")\n",
    "from langchain.agents.openai_functions_agent.base import OpenAIFunctionsAgent\n",
    "from langchain.memory.token_buffer import ConversationTokenBufferMemory\n",
    "from langchain.tools.base import BaseTool\n",
    "\n",
    "\n",
    "def _get_default_system_message() -> SystemMessage:\n",
    "    return SystemMessage(\n",
    "        content=(\n",
    "            \"Do your best to answer the questions. \"\n",
    "            \"Feel free to use any tools available to look up \"\n",
    "            \"relevant information, only if necessary\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def create_conversational_retrieval_agent_dev(\n",
    "    llm: BaseLanguageModel,\n",
    "    tools: List[BaseTool],\n",
    "    remember_intermediate_steps: bool = True,\n",
    "    memory_key: str = \"chat_history\",\n",
    "    system_message: Optional[SystemMessage] = None,\n",
    "    verbose: bool = False,\n",
    "    max_token_limit: int = 2000,\n",
    "    **kwargs: Any,\n",
    ") -> AgentExecutor:\n",
    "    \"\"\"A convenience method for creating a conversational retrieval agent.\n",
    "\n",
    "    Args:\n",
    "        llm: The language model to use, should be ChatOpenAI\n",
    "        tools: A list of tools the agent has access to\n",
    "        remember_intermediate_steps: Whether the agent should remember intermediate\n",
    "            steps or not. Intermediate steps refer to prior action/observation\n",
    "            pairs from previous questions. The benefit of remembering these is if\n",
    "            there is relevant information in there, the agent can use it to answer\n",
    "            follow up questions. The downside is it will take up more tokens.\n",
    "        memory_key: The name of the memory key in the prompt.\n",
    "        system_message: The system message to use. By default, a basic one will\n",
    "            be used.\n",
    "        verbose: Whether or not the final AgentExecutor should be verbose or not,\n",
    "            defaults to False.\n",
    "        max_token_limit: The max number of tokens to keep around in memory.\n",
    "            Defaults to 2000.\n",
    "\n",
    "    Returns:\n",
    "        An agent executor initialized appropriately\n",
    "    \"\"\"\n",
    "\n",
    "    if remember_intermediate_steps:\n",
    "        memory: BaseMemory = AgentTokenBufferMemory(\n",
    "            memory_key=memory_key, llm=llm, max_token_limit=max_token_limit\n",
    "        )\n",
    "    else:\n",
    "        memory = ConversationTokenBufferMemory(\n",
    "            memory_key=memory_key,\n",
    "            return_messages=True,\n",
    "            output_key=\"output\",\n",
    "            llm=llm,\n",
    "            max_token_limit=max_token_limit,\n",
    "        )\n",
    "\n",
    "    _system_message = system_message or _get_default_system_message()\n",
    "    \n",
    "    print(MessagesPlaceholder(variable_name=memory_key))\n",
    "    prompt = OpenAIFunctionsAgent.create_prompt(\n",
    "        system_message=_system_message,\n",
    "        extra_prompt_messages=[MessagesPlaceholder(variable_name=memory_key)],\n",
    "    )\n",
    "    print(\"prompt\", prompt)\n",
    "    agent = OpenAIFunctionsAgentDev(llm=llm, tools=tools, prompt=prompt)\n",
    "    return AgentExecutor(\n",
    "        agent=agent,\n",
    "        tools=tools,\n",
    "        memory=memory,\n",
    "        verbose=verbose,\n",
    "        return_intermediate_steps=remember_intermediate_steps,\n",
    "        **kwargs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "1d9483f0-3835-4337-ba75-04570a859160",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable_name='chat_history'\n",
      "prompt input_variables=['agent_scratchpad', 'chat_history', 'input'] input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]} messages=[SystemMessage(content='Do your best to answer the questions. Feel free to use any tools available to look up relevant information, only if necessary'), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')]\n",
      "variable_name='chat_history'\n",
      "prompt input_variables=['agent_scratchpad', 'chat_history', 'input'] input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]} messages=[SystemMessage(content='Do your best to answer the questions. Feel free to use any tools available to look up relevant information, only if necessary'), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')]\n"
     ]
    }
   ],
   "source": [
    "agent_executor_mistral = create_conversational_retrieval_agent_dev(mistral_llm, tools, verbose=True)\n",
    "agent_executor_openai = create_conversational_retrieval_agent_dev(llm, tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "00242e04-65f1-4518-902b-6dedb920b5c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.01` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "intermediate_steps []\n",
      "agent scratchpad []\n",
      "with functions True\n",
      "messages [SystemMessage(content='Do your best to answer the questions. Feel free to use any tools available to look up relevant information, only if necessary'), HumanMessage(content='how is gus edwards doing?')]\n",
      "format tools [{'name': 'football_search', 'description': 'Searches and returns documents regarding fantasy football.', 'parameters': {'title': 'RetrieverInput', 'type': 'object', 'properties': {'query': {'title': 'Query', 'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query']}}]\n",
      "functions [{'name': 'football_search', 'description': 'Searches and returns documents regarding fantasy football.', 'parameters': {'title': 'RetrieverInput', 'type': 'object', 'properties': {'query': {'title': 'Query', 'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query']}}]\n",
      "callbacks <langchain_core.callbacks.manager.CallbackManager object at 0x7fd15140f520>\n",
      "format tools [{'name': 'football_search', 'description': 'Searches and returns documents regarding fantasy football.', 'parameters': {'title': 'RetrieverInput', 'type': 'object', 'properties': {'query': {'title': 'Query', 'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query']}}]\n",
      "predicted_message <class 'langchain_core.messages.ai.AIMessage'> content=\"\\nAI: I don't have real-time data on Gus Edwards' health or wellbeing. However, as of my last update, he appears to be in good health and active on social media. Please check the latest sources for the most accurate information.\"\n",
      "agent_decision return_values={'output': \"\\nAI: I don't have real-time data on Gus Edwards' health or wellbeing. However, as of my last update, he appears to be in good health and active on social media. Please check the latest sources for the most accurate information.\"} log=\"\\nAI: I don't have real-time data on Gus Edwards' health or wellbeing. However, as of my last update, he appears to be in good health and active on social media. Please check the latest sources for the most accurate information.\"\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "AI: I don't have real-time data on Gus Edwards' health or wellbeing. However, as of my last update, he appears to be in good health and active on social media. Please check the latest sources for the most accurate information.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "result = agent_executor_mistral({\"input\": \"how is gus edwards doing?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cf8a06-5e99-4ea9-b1eb-8211b68fed31",
   "metadata": {},
   "source": [
    "```\n",
    "predicted_message content=\"\\nAI: I don't have real-time data on Gus Edwards' health or wellbeing. However, as of my last update, he appears to be in good health and active on social media. Please check the latest sources for the most accurate information.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2040e43-adad-4567-9147-2e379923952c",
   "metadata": {},
   "source": [
    "```predicted_message content='' additional_kwargs={'function_call': {'arguments': '{\\n  \"query\": \"Gus Edwards\"\\n}', 'name': 'football_search'}}\n",
    "agent_decision tool='football_search' tool_input={'query': 'Gus Edwards'} log=\"\\nInvoking: `football_search` with `{'query': 'Gus Edwards'}`\\n\\n\\n\" message_log=[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"query\": \"Gus Edwards\"\\n}', 'name': 'football_search'}})]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "bc3bfce0-6f44-4d9c-9dc3-134d31833f7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages.ai import AIMessage\n",
    "predicted_message = AIMessage(\n",
    "    content='',\n",
    "    additional_kwargs={\n",
    "        'function_call': {\n",
    "            'arguments': {\n",
    "                'query': 'Gus Edwards'\n",
    "            },\n",
    "            'name': 'football_search'\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "fb88dbbe-5e05-4eab-9c88-5529af1ccaf8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': {'query': 'Gus Edwards'}, 'name': 'football_search'}})"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "01a2c4cf-f542-4c8b-b8ef-df676378a7ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "intermediate_steps []\n",
      "agent scratchpad []\n",
      "with functions True\n",
      "messages [SystemMessage(content='Do your best to answer the questions. Feel free to use any tools available to look up relevant information, only if necessary'), HumanMessage(content='how is gus edwards doing?')]\n",
      "format tools [{'name': 'football_search', 'description': 'Searches and returns documents regarding fantasy football.', 'parameters': {'title': 'RetrieverInput', 'type': 'object', 'properties': {'query': {'title': 'Query', 'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query']}}]\n",
      "functions [{'name': 'football_search', 'description': 'Searches and returns documents regarding fantasy football.', 'parameters': {'title': 'RetrieverInput', 'type': 'object', 'properties': {'query': {'title': 'Query', 'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query']}}]\n",
      "callbacks <langchain_core.callbacks.manager.CallbackManager object at 0x7fd1514a5f30>\n",
      "predicted_message <class 'langchain_core.messages.ai.AIMessage'> content='' additional_kwargs={'function_call': {'arguments': {'query': 'Gus Edwards'}, 'name': 'football_search'}}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'strip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[234], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43magent_executor_openai\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhow is gus edwards doing?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py:312\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    311\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    313\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    314\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    315\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    316\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py:306\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    299\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    300\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    301\u001b[0m     inputs,\n\u001b[1;32m    302\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[1;32m    303\u001b[0m )\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 306\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    309\u001b[0m     )\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    311\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/agents/agent.py:1312\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m   1310\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m   1311\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[0;32m-> 1312\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[1;32m   1321\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[1;32m   1322\u001b[0m         )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/agents/agent.py:1038\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[1;32m   1030\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1031\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1035\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1036\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[0;32m-> 1038\u001b[0m         [\n\u001b[1;32m   1039\u001b[0m             a\n\u001b[1;32m   1040\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_next_step(\n\u001b[1;32m   1041\u001b[0m                 name_to_tool_map,\n\u001b[1;32m   1042\u001b[0m                 color_mapping,\n\u001b[1;32m   1043\u001b[0m                 inputs,\n\u001b[1;32m   1044\u001b[0m                 intermediate_steps,\n\u001b[1;32m   1045\u001b[0m                 run_manager,\n\u001b[1;32m   1046\u001b[0m             )\n\u001b[1;32m   1047\u001b[0m         ]\n\u001b[1;32m   1048\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/agents/agent.py:1038\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[1;32m   1030\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1031\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1035\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1036\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[0;32m-> 1038\u001b[0m         [\n\u001b[1;32m   1039\u001b[0m             a\n\u001b[1;32m   1040\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_next_step(\n\u001b[1;32m   1041\u001b[0m                 name_to_tool_map,\n\u001b[1;32m   1042\u001b[0m                 color_mapping,\n\u001b[1;32m   1043\u001b[0m                 inputs,\n\u001b[1;32m   1044\u001b[0m                 intermediate_steps,\n\u001b[1;32m   1045\u001b[0m                 run_manager,\n\u001b[1;32m   1046\u001b[0m             )\n\u001b[1;32m   1047\u001b[0m         ]\n\u001b[1;32m   1048\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/agents/agent.py:1066\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1063\u001b[0m     intermediate_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_intermediate_steps(intermediate_steps)\n\u001b[1;32m   1065\u001b[0m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[0;32m-> 1066\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1072\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "Cell \u001b[0;32mIn[230], line 127\u001b[0m, in \u001b[0;36mOpenAIFunctionsAgentDev.plan\u001b[0;34m(self, intermediate_steps, callbacks, with_functions, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    123\u001b[0m     predicted_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mpredict_messages(\n\u001b[1;32m    124\u001b[0m         messages,\n\u001b[1;32m    125\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    126\u001b[0m     )\n\u001b[0;32m--> 127\u001b[0m agent_decision \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAIFunctionsAgentOutputParser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_ai_message\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredicted_message\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent_decision\u001b[39m\u001b[38;5;124m\"\u001b[39m, agent_decision)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m agent_decision\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/agents/output_parsers/openai_functions.py:44\u001b[0m, in \u001b[0;36mOpenAIFunctionsAgentOutputParser._parse_ai_message\u001b[0;34m(message)\u001b[0m\n\u001b[1;32m     42\u001b[0m function_name \u001b[38;5;241m=\u001b[39m function_call[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[43mfunction_call\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marguments\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip\u001b[49m()) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;66;03m# OpenAI returns an empty string for functions containing no args\u001b[39;00m\n\u001b[1;32m     46\u001b[0m         _tool_input \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;66;03m# otherwise it returns a json object\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'strip'"
     ]
    }
   ],
   "source": [
    "result = agent_executor_openai({\"input\": \"how is gus edwards doing?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "a5a71c55-537d-4fe5-9ca2-8f37d6aeafba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /opt/conda/lib/python3.10/site-packages (0.5.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2023.10.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2023.11.17)\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1107b6-5bbb-4cb7-9c93-fff6ec6aeacd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-gpu.m114",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-gpu:m114"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
