{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "a3f04bcf-f76c-4172-82ea-76aa683a1305",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5c35f445-7e40-4f56-8a1b-be004736abc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftModel\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_transformers import Html2TextTransformer\n",
    "from langchain.document_loaders import AsyncChromiumLoader\n",
    "\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "10517235-0939-474a-9825-0381a29decce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftModel\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_transformers import Html2TextTransformer\n",
    "from langchain.document_loaders import AsyncChromiumLoader\n",
    "\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "607a21d4-c7d9-47d8-aed1-79eac725f4c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Your GPU supports bfloat16: accelerate training with bf16=True\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08b59e26b1ba4cb58ce2196d95406b14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#################################################################\n",
    "# Tokenizer\n",
    "#################################################################\n",
    "\n",
    "model_name='mistralai/Mistral-7B-Instruct-v0.1'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "#################################################################\n",
    "# bitsandbytes parameters\n",
    "#################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "#################################################################\n",
    "# Set up quantization config\n",
    "#################################################################\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "#################################################################\n",
    "# Load pre-trained config\n",
    "#################################################################\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "f9876d71-4271-4d41-b259-e15ce74da071",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_generation_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.0,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=1000,\n",
    ")\n",
    "\n",
    "mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "30c843ff-ae39-4beb-82c6-e500d0961f5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 4148, which is longer than the specified 300\n",
      "Created a chunk of size 500, which is longer than the specified 300\n",
      "Created a chunk of size 365, which is longer than the specified 300\n",
      "Created a chunk of size 312, which is longer than the specified 300\n",
      "Created a chunk of size 515, which is longer than the specified 300\n",
      "Created a chunk of size 584, which is longer than the specified 300\n",
      "Created a chunk of size 1119, which is longer than the specified 300\n",
      "Created a chunk of size 4148, which is longer than the specified 300\n",
      "Created a chunk of size 567, which is longer than the specified 300\n",
      "Created a chunk of size 316, which is longer than the specified 300\n",
      "Created a chunk of size 449, which is longer than the specified 300\n",
      "Created a chunk of size 479, which is longer than the specified 300\n",
      "Created a chunk of size 533, which is longer than the specified 300\n",
      "Created a chunk of size 446, which is longer than the specified 300\n",
      "Created a chunk of size 317, which is longer than the specified 300\n",
      "Created a chunk of size 4148, which is longer than the specified 300\n",
      "Created a chunk of size 450, which is longer than the specified 300\n",
      "Created a chunk of size 462, which is longer than the specified 300\n",
      "Created a chunk of size 469, which is longer than the specified 300\n",
      "Created a chunk of size 432, which is longer than the specified 300\n",
      "Created a chunk of size 380, which is longer than the specified 300\n",
      "Created a chunk of size 4148, which is longer than the specified 300\n",
      "Created a chunk of size 422, which is longer than the specified 300\n",
      "Created a chunk of size 498, which is longer than the specified 300\n",
      "Created a chunk of size 413, which is longer than the specified 300\n",
      "Created a chunk of size 456, which is longer than the specified 300\n",
      "Created a chunk of size 404, which is longer than the specified 300\n",
      "Created a chunk of size 380, which is longer than the specified 300\n",
      "Created a chunk of size 346, which is longer than the specified 300\n",
      "Created a chunk of size 4148, which is longer than the specified 300\n",
      "Created a chunk of size 403, which is longer than the specified 300\n",
      "Created a chunk of size 541, which is longer than the specified 300\n",
      "Created a chunk of size 324, which is longer than the specified 300\n",
      "Created a chunk of size 305, which is longer than the specified 300\n",
      "Created a chunk of size 333, which is longer than the specified 300\n",
      "Created a chunk of size 457, which is longer than the specified 300\n",
      "Created a chunk of size 421, which is longer than the specified 300\n",
      "Created a chunk of size 528, which is longer than the specified 300\n",
      "Created a chunk of size 592, which is longer than the specified 300\n",
      "Created a chunk of size 633, which is longer than the specified 300\n",
      "Created a chunk of size 303, which is longer than the specified 300\n",
      "Created a chunk of size 658, which is longer than the specified 300\n",
      "Created a chunk of size 520, which is longer than the specified 300\n",
      "Created a chunk of size 438, which is longer than the specified 300\n",
      "Created a chunk of size 333, which is longer than the specified 300\n",
      "Created a chunk of size 581, which is longer than the specified 300\n",
      "Created a chunk of size 432, which is longer than the specified 300\n",
      "Created a chunk of size 574, which is longer than the specified 300\n",
      "Created a chunk of size 741, which is longer than the specified 300\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Articles to index\n",
    "articles = [\"https://www.fantasypros.com/2023/11/rival-fantasy-nfl-week-10/\",\n",
    "            \"https://www.fantasypros.com/2023/11/5-stats-to-know-before-setting-your-fantasy-lineup-week-10/\",\n",
    "            \"https://www.fantasypros.com/2023/11/nfl-week-10-sleeper-picks-player-predictions-2023/\",\n",
    "            \"https://www.fantasypros.com/2023/11/nfl-dfs-week-10-stacking-advice-picks-2023-fantasy-football/\",\n",
    "            \"https://www.fantasypros.com/2023/11/players-to-buy-low-sell-high-trade-advice-2023-fantasy-football/\"]\n",
    "\n",
    "# Scrapes the blogs above\n",
    "loader = AsyncChromiumLoader(articles)\n",
    "docs = loader.load()\n",
    "\n",
    "# Converts HTML to plain text \n",
    "html2text = Html2TextTransformer()\n",
    "docs_transformed = html2text.transform_documents(docs)\n",
    "\n",
    "# Chunk text\n",
    "text_splitter = CharacterTextSplitter(chunk_size=300, \n",
    "                                      chunk_overlap=0)\n",
    "chunked_documents = text_splitter.split_documents(docs_transformed)\n",
    "\n",
    "# Load chunked documents into the FAISS index\n",
    "db = FAISS.from_documents(chunked_documents, \n",
    "                          HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2'))\n",
    "\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "bc465cc0-8851-4171-9763-88d4571c70e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents.agent_toolkits import create_retriever_tool\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "class RetrieverInput(BaseModel):\n",
    "    query: str = Field(description=\"query to look up in retriever\")\n",
    "\n",
    "tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"football_search\",\n",
    "    \"Searches and returns documents regarding fantasy football.\",\n",
    ")\n",
    "tools = [tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "591b57b9-14da-4eda-ab40-872a30edd3ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, openai_api_key = \"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "8f3e0f05-c76d-4fb9-8973-6f5ad6713945",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import *\n",
    "from langchain_core.callbacks import (\n",
    "    AsyncCallbackManagerForLLMRun,\n",
    "    CallbackManagerForLLMRun,\n",
    ")\n",
    "from langchain_core.language_models import LanguageModelInput\n",
    "from langchain_core.language_models.chat_models import (\n",
    "    BaseChatModel,\n",
    "    agenerate_from_stream,\n",
    "    generate_from_stream,\n",
    ")\n",
    "from langchain_core.language_models.llms import create_base_retry_decorator\n",
    "from langchain_core.messages import (\n",
    "    AIMessageChunk,\n",
    "    BaseMessage,\n",
    "    BaseMessageChunk,\n",
    "    ChatMessageChunk,\n",
    "    FunctionMessageChunk,\n",
    "    HumanMessageChunk,\n",
    "    SystemMessageChunk,\n",
    "    ToolMessageChunk,\n",
    ")\n",
    "from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, root_validator\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.utils import (\n",
    "    get_from_dict_or_env,\n",
    "    get_pydantic_field_names,\n",
    ")\n",
    "\n",
    "from langchain_community.adapters.openai import (\n",
    "    convert_dict_to_message,\n",
    "    convert_message_to_dict,\n",
    ")\n",
    "from langchain_community.utils.openai import is_openai_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "ca0be89b-30b4-4000-889a-a65593788b32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ChatOpenAIDev(ChatOpenAI):\n",
    "    \n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        stream: Optional[bool] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        should_stream = stream if stream is not None else self.streaming\n",
    "        if should_stream:\n",
    "            stream_iter = self._stream(\n",
    "                messages, stop=stop, run_manager=run_manager, **kwargs\n",
    "            )\n",
    "            return generate_from_stream(stream_iter)\n",
    "        message_dicts, params = self._create_message_dicts(messages, stop)\n",
    "        print(kwargs)\n",
    "        print({\"stream\": stream} if stream is not None else {})\n",
    "        print(params)\n",
    "        params = {\n",
    "            **params,\n",
    "            **({\"stream\": stream} if stream is not None else {}),\n",
    "            **kwargs,\n",
    "        }\n",
    "        print()\n",
    "        print(params)\n",
    "        response = self.completion_with_retry(\n",
    "            messages=message_dicts, run_manager=run_manager, **params\n",
    "        )\n",
    "        \n",
    "        print(response)\n",
    "        return self._create_chat_result(response)\n",
    "\n",
    "    def bind_functions(\n",
    "    self,\n",
    "    functions: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable]],\n",
    "    function_call: Optional[str] = None,\n",
    "    **kwargs: Any,\n",
    "    ) -> Runnable[LanguageModelInput, BaseMessage]:\n",
    "        \"\"\"Bind functions (and other objects) to this chat model.\n",
    "\n",
    "        Args:\n",
    "            functions: A list of function definitions to bind to this chat model.\n",
    "                Can be  a dictionary, pydantic model, or callable. Pydantic\n",
    "                models and callables will be automatically converted to\n",
    "                their schema dictionary representation.\n",
    "            function_call: Which function to require the model to call.\n",
    "                Must be the name of the single provided function or\n",
    "                \"auto\" to automatically determine which function to call\n",
    "                (if any).\n",
    "            kwargs: Any additional parameters to pass to the\n",
    "                :class:`~langchain.runnable.Runnable` constructor.\n",
    "        \"\"\"\n",
    "        from langchain.chains.openai_functions.base import convert_to_openai_function\n",
    "\n",
    "        formatted_functions = [convert_to_openai_function(fn) for fn in functions]\n",
    "        if function_call is not None:\n",
    "            if len(formatted_functions) != 1:\n",
    "                raise ValueError(\n",
    "                    \"When specifying `function_call`, you must provide exactly one \"\n",
    "                    \"function.\"\n",
    "                )\n",
    "            if formatted_functions[0][\"name\"] != function_call:\n",
    "                raise ValueError(\n",
    "                    f\"Function call {function_call} was specified, but the only \"\n",
    "                    f\"provided function was {formatted_functions[0]['name']}.\"\n",
    "                )\n",
    "            function_call_ = {\"name\": function_call}\n",
    "            kwargs = {**kwargs, \"function_call\": function_call_}\n",
    "\n",
    "            print(formatted_functions)\n",
    "            print(kwargs)\n",
    "        return super().bind(\n",
    "            functions=formatted_functions,\n",
    "            **kwargs,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "df5cc815-a569-4deb-9805-5b75d2131d5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAIDev(temperature=0, openai_api_key = \"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "a37aa7e0-3792-4747-bec1-11297e63b4b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'functions': [{'name': 'football_search', 'description': 'Searches and returns documents regarding fantasy football.', 'parameters': {'title': 'RetrieverInput', 'type': 'object', 'properties': {'query': {'title': 'Query', 'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query']}}]}\n",
      "{}\n",
      "{'model': 'gpt-3.5-turbo', 'stream': False, 'n': 1, 'temperature': 0.0}\n",
      "\n",
      "{'model': 'gpt-3.5-turbo', 'stream': False, 'n': 1, 'temperature': 0.0, 'functions': [{'name': 'football_search', 'description': 'Searches and returns documents regarding fantasy football.', 'parameters': {'title': 'RetrieverInput', 'type': 'object', 'properties': {'query': {'title': 'Query', 'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query']}}]}\n",
      "ChatCompletion(id='chatcmpl-8YOB7Te3sp29Ev9ARQMzyziydzw2A', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\\n  \"query\": \"Gibbs fantasy football performance\"\\n}', name='football_search'), tool_calls=None))], created=1703206785, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=21, prompt_tokens=85, total_tokens=106))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"query\": \"Gibbs fantasy football performance\"\\n}', 'name': 'football_search'}})"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict_messages(\n",
    "                messages,\n",
    "                functions=functions,\n",
    "                callbacks=None,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "00471266-6752-4206-b941-3049750fdbec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-8YO61ipZ0QE8OGZsueNkb9cW9B5TK', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"As an AI language model, I don't have real-time information. However, as of my last knowledge, Gus Edwards is a professional American football running back who plays for the Baltimore Ravens in the NFL. He has been a reliable and productive player for the Ravens, known for his strong running style and ability to break tackles. If you are looking for the most up-to-date information on Gus Edwards, I would recommend checking sports news websites or following the Baltimore Ravens' official channels.\", role='assistant', function_call=None, tool_calls=None))], created=1703206469, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=96, prompt_tokens=15, total_tokens=111))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"As an AI language model, I don't have real-time information. However, as of my last knowledge, Gus Edwards is a professional American football running back who plays for the Baltimore Ravens in the NFL. He has been a reliable and productive player for the Ravens, known for his strong running style and ability to break tackles. If you are looking for the most up-to-date information on Gus Edwards, I would recommend checking sports news websites or following the Baltimore Ravens' official channels.\")"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"how is gus edwards doing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "4e025eb4-123b-405a-8fa2-2a1350380e14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nUser 1: I\\'m not sure what you mean by \"doing\". He\\'s a musician and producer. He\\'s been working on his own music for a while now, but he also works with other artists as a producer.\\n\\nHe\\'s also involved in the music industry in general, working with various companies and organizations to help promote and support new talent.\\n\\nI don\\'t know if that answers your question. If you have any more specific questions about him or his work, feel free to ask!'"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistral_llm.invoke(\"how is gus edwards doing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "91874fe7-bc0e-4be5-ade8-1bdbc2d2c837",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "b2ede358-0a65-417a-b169-c700fcf06e22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Module implements an agent that uses OpenAI's APIs function enabled API.\"\"\"\n",
    "from typing import Any, List, Optional, Sequence, Tuple, Union\n",
    "\n",
    "from langchain_core.agents import AgentAction, AgentFinish\n",
    "from langchain_core.language_models import BaseLanguageModel\n",
    "from langchain_core.messages import (\n",
    "    BaseMessage,\n",
    "    SystemMessage,\n",
    "    HumanMessage\n",
    ")\n",
    "from langchain_core.prompts import BasePromptTemplate\n",
    "from langchain_core.prompts.chat import (\n",
    "    BaseMessagePromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "from langchain_core.pydantic_v1 import root_validator\n",
    "from langchain_core.tools import BaseTool\n",
    "\n",
    "from langchain.agents import BaseSingleActionAgent\n",
    "from langchain.agents.format_scratchpad.openai_functions import (\n",
    "    format_to_openai_function_messages,\n",
    ")\n",
    "from langchain.agents.output_parsers.openai_functions import (\n",
    "    OpenAIFunctionsAgentOutputParser,\n",
    ")\n",
    "from langchain.callbacks.base import BaseCallbackManager\n",
    "from langchain.callbacks.manager import Callbacks\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "\n",
    "\n",
    "class OpenAIFunctionsAgentDev(BaseSingleActionAgent):\n",
    "    \"\"\"An Agent driven by OpenAIs function powered API.\n",
    "\n",
    "    Args:\n",
    "        llm: This should be an instance of ChatOpenAI, specifically a model\n",
    "            that supports using `functions`.\n",
    "        tools: The tools this agent has access to.\n",
    "        prompt: The prompt for this agent, should support agent_scratchpad as one\n",
    "            of the variables. For an easy way to construct this prompt, use\n",
    "            `OpenAIFunctionsAgent.create_prompt(...)`\n",
    "    \"\"\"\n",
    "\n",
    "    llm: BaseLanguageModel\n",
    "    tools: Sequence[BaseTool]\n",
    "    prompt: BasePromptTemplate\n",
    "\n",
    "    def get_allowed_tools(self) -> List[str]:\n",
    "        \"\"\"Get allowed tools.\"\"\"\n",
    "        return [t.name for t in self.tools]\n",
    "\n",
    "    @root_validator\n",
    "    def validate_prompt(cls, values: dict) -> dict:\n",
    "        prompt: BasePromptTemplate = values[\"prompt\"]\n",
    "        if \"agent_scratchpad\" not in prompt.input_variables:\n",
    "            raise ValueError(\n",
    "                \"`agent_scratchpad` should be one of the variables in the prompt, \"\n",
    "                f\"got {prompt.input_variables}\"\n",
    "            )\n",
    "        return values\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        \"\"\"Get input keys. Input refers to user input here.\"\"\"\n",
    "        return [\"input\"]\n",
    "\n",
    "    @property\n",
    "    def functions(self) -> List[dict]:\n",
    "        print(\"format tools\", [dict(format_tool_to_openai_function(t)) for t in self.tools])\n",
    "        return [dict(format_tool_to_openai_function(t)) for t in self.tools]\n",
    "\n",
    "    def plan(\n",
    "        self,\n",
    "        intermediate_steps: List[Tuple[AgentAction, str]],\n",
    "        callbacks: Callbacks = None,\n",
    "        with_functions: bool = True,\n",
    "        **kwargs: Any,\n",
    "    ) -> Union[AgentAction, AgentFinish]:\n",
    "        \"\"\"Given input, decided what to do.\n",
    "\n",
    "        Args:\n",
    "            intermediate_steps: Steps the LLM has taken to date, along with observations\n",
    "            **kwargs: User inputs.\n",
    "\n",
    "        Returns:\n",
    "            Action specifying what tool to use.\n",
    "        \"\"\"\n",
    "        print(\"intermediate_steps\", intermediate_steps)\n",
    "        \n",
    "        agent_scratchpad = format_to_openai_function_messages(intermediate_steps)\n",
    "        print(\"agent scratchpad\", agent_scratchpad)\n",
    "        selected_inputs = {\n",
    "            k: kwargs[k] for k in self.prompt.input_variables if k != \"agent_scratchpad\"\n",
    "        }\n",
    "        full_inputs = dict(**selected_inputs, agent_scratchpad=agent_scratchpad)\n",
    "        prompt = self.prompt.format_prompt(**full_inputs)\n",
    "        messages = prompt.to_messages()\n",
    "        print(\"with functions\", with_functions)\n",
    "        if with_functions:\n",
    "            \n",
    "            print(\"messages\", messages)\n",
    "            print(\"functions\", self.functions)\n",
    "            print(\"callbacks\", callbacks)\n",
    "            predicted_message = self.llm.predict_messages(\n",
    "                messages,\n",
    "                functions=self.functions,\n",
    "                callbacks=callbacks,\n",
    "            )\n",
    "\n",
    "            arguments_json = json.dumps({'query': 'Gus Edwards'})\n",
    "\n",
    "            predicted_message2 = AIMessage(\n",
    "                content='',\n",
    "                additional_kwargs={\n",
    "                    'function_call': {\n",
    "                        'arguments': arguments_json,\n",
    "                        'name': 'football_search'\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "\n",
    "            print(\"predicted_message\", type(predicted_message), predicted_message, )\n",
    "            print(\"predicted_message2 \", type(predicted_message2 ), predicted_message2 , )\n",
    "        else:\n",
    "            predicted_message = self.llm.predict_messages(\n",
    "                messages,\n",
    "                callbacks=callbacks,\n",
    "            )\n",
    "        agent_decision = OpenAIFunctionsAgentOutputParser._parse_ai_message(\n",
    "            predicted_message\n",
    "        )\n",
    "        \n",
    "        print(\"agent_decision\", agent_decision)\n",
    "        return agent_decision\n",
    "\n",
    "    async def aplan(\n",
    "        self,\n",
    "        intermediate_steps: List[Tuple[AgentAction, str]],\n",
    "        callbacks: Callbacks = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Union[AgentAction, AgentFinish]:\n",
    "        \"\"\"Given input, decided what to do.\n",
    "\n",
    "        Args:\n",
    "            intermediate_steps: Steps the LLM has taken to date,\n",
    "                along with observations\n",
    "            **kwargs: User inputs.\n",
    "\n",
    "        Returns:\n",
    "            Action specifying what tool to use.\n",
    "        \"\"\"\n",
    "        print()\n",
    "        print(\"intermediate steps\", intermediate_steps)\n",
    "        agent_scratchpad = format_to_openai_function_messages(intermediate_steps)\n",
    "        \n",
    "        print(\"agent scratchpad\", agent_scratchpad)\n",
    "        selected_inputs = {\n",
    "            k: kwargs[k] for k in self.prompt.input_variables if k != \"agent_scratchpad\"\n",
    "        }\n",
    "        full_inputs = dict(**selected_inputs, agent_scratchpad=agent_scratchpad)\n",
    "        prompt = self.prompt.format_prompt(**full_inputs)\n",
    "        messages = prompt.to_messages()\n",
    "        print(\"messages aplan\", messages)\n",
    "        predicted_message = await self.llm.apredict_messages(\n",
    "            messages, functions=self.functions, callbacks=callbacks\n",
    "        )\n",
    "        \n",
    "        # print(\"predicted_message\", predicted_message)\n",
    "        \n",
    "        agent_decision = OpenAIFunctionsAgentOutputParser._parse_ai_message(\n",
    "            predicted_message\n",
    "        )\n",
    "        \n",
    "        # print(\"agent_decision\",agent_decision)\n",
    "        return agent_decision\n",
    "\n",
    "    def return_stopped_response(\n",
    "        self,\n",
    "        early_stopping_method: str,\n",
    "        intermediate_steps: List[Tuple[AgentAction, str]],\n",
    "        **kwargs: Any,\n",
    "    ) -> AgentFinish:\n",
    "        \"\"\"Return response when agent has been stopped due to max iterations.\"\"\"\n",
    "        print()\n",
    "        print(\"intermediate steps return stopped\", intermediate_steps)\n",
    "        print(\"early stopping\", early_stopping_method)\n",
    "        print()\n",
    "        if early_stopping_method == \"force\":\n",
    "            # `force` just returns a constant string\n",
    "            return AgentFinish(\n",
    "                {\"output\": \"Agent stopped due to iteration limit or time limit.\"}, \"\"\n",
    "            )\n",
    "        elif early_stopping_method == \"generate\":\n",
    "            # Generate does one final forward pass\n",
    "            agent_decision = self.plan(\n",
    "                intermediate_steps, with_functions=False, **kwargs\n",
    "            )\n",
    "            \n",
    "            # print(\"agent decision 2\", agent_decision)\n",
    "            if isinstance(agent_decision, AgentFinish):\n",
    "                return agent_decision\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"got AgentAction with no functions provided: {agent_decision}\"\n",
    "                )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"early_stopping_method should be one of `force` or `generate`, \"\n",
    "                f\"got {early_stopping_method}\"\n",
    "            )\n",
    "\n",
    "    @classmethod\n",
    "    def create_prompt(\n",
    "        cls,\n",
    "        system_message: Optional[SystemMessage] = SystemMessage(\n",
    "            content=\"You are a helpful AI assistant.\"\n",
    "        ),\n",
    "        extra_prompt_messages: Optional[List[BaseMessagePromptTemplate]] = None,\n",
    "    ) -> BasePromptTemplate:\n",
    "        \"\"\"Create prompt for this agent.\n",
    "\n",
    "        Args:\n",
    "            system_message: Message to use as the system message that will be the\n",
    "                first in the prompt.\n",
    "            extra_prompt_messages: Prompt messages that will be placed between the\n",
    "                system message and the new human input.\n",
    "\n",
    "        Returns:\n",
    "            A prompt template to pass into this agent.\n",
    "        \"\"\"\n",
    "        _prompts = extra_prompt_messages or []\n",
    "        messages: List[Union[BaseMessagePromptTemplate, BaseMessage]]\n",
    "        if system_message:\n",
    "            messages = [system_message]\n",
    "        else:\n",
    "            messages = []\n",
    "\n",
    "        messages.extend(\n",
    "            [\n",
    "                *_prompts,\n",
    "                HumanMessagePromptTemplate.from_template(\"{input}\"),\n",
    "                MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "            ]\n",
    "        )\n",
    "        return ChatPromptTemplate(messages=messages)\n",
    "\n",
    "    @classmethod\n",
    "    def from_llm_and_tools(\n",
    "        cls,\n",
    "        llm: BaseLanguageModel,\n",
    "        tools: Sequence[BaseTool],\n",
    "        callback_manager: Optional[BaseCallbackManager] = None,\n",
    "        extra_prompt_messages: Optional[List[BaseMessagePromptTemplate]] = None,\n",
    "        system_message: Optional[SystemMessage] = SystemMessage(\n",
    "            content=\"You are a helpful AI assistant.\"\n",
    "        ),\n",
    "        **kwargs: Any,\n",
    "    ) -> BaseSingleActionAgent:\n",
    "        \"\"\"Construct an agent from an LLM and tools.\"\"\"\n",
    "        prompt = cls.create_prompt(\n",
    "            extra_prompt_messages=extra_prompt_messages,\n",
    "            system_message=system_message,\n",
    "        )\n",
    "        return cls(\n",
    "            llm=llm,\n",
    "            prompt=prompt,\n",
    "            tools=tools,\n",
    "            callback_manager=callback_manager,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "3c44fd94-03b6-4ad5-8135-15946e3d6a31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from agent_executor import AgentExecutorDev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "2e1857fe-5b08-4d60-860a-33ad1f44f1ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Any, List, Optional  # noqa: E501\n",
    "\n",
    "from langchain_core.language_models import BaseLanguageModel\n",
    "from langchain_core.memory import BaseMemory\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts.chat import MessagesPlaceholder\n",
    "\n",
    "from langchain.agents.agent import AgentExecutor\n",
    "from langchain.agents.openai_functions_agent.agent_token_buffer_memory import (\n",
    "    AgentTokenBufferMemory,\n",
    ")\n",
    "from langchain.agents.openai_functions_agent.base import OpenAIFunctionsAgent\n",
    "from langchain.memory.token_buffer import ConversationTokenBufferMemory\n",
    "from langchain.tools.base import BaseTool\n",
    "\n",
    "\n",
    "def _get_default_system_message() -> SystemMessage:\n",
    "    return SystemMessage(\n",
    "        content=(\n",
    "            \"Do your best to answer the questions. \"\n",
    "            \"Feel free to use any tools available to look up \"\n",
    "            \"relevant information, only if necessary\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def create_conversational_retrieval_agent_dev(\n",
    "    llm: BaseLanguageModel,\n",
    "    tools: List[BaseTool],\n",
    "    remember_intermediate_steps: bool = True,\n",
    "    memory_key: str = \"chat_history\",\n",
    "    system_message: Optional[SystemMessage] = None,\n",
    "    verbose: bool = False,\n",
    "    max_token_limit: int = 2000,\n",
    "    **kwargs: Any,\n",
    ") -> AgentExecutor:\n",
    "    \"\"\"A convenience method for creating a conversational retrieval agent.\n",
    "\n",
    "    Args:\n",
    "        llm: The language model to use, should be ChatOpenAI\n",
    "        tools: A list of tools the agent has access to\n",
    "        remember_intermediate_steps: Whether the agent should remember intermediate\n",
    "            steps or not. Intermediate steps refer to prior action/observation\n",
    "            pairs from previous questions. The benefit of remembering these is if\n",
    "            there is relevant information in there, the agent can use it to answer\n",
    "            follow up questions. The downside is it will take up more tokens.\n",
    "        memory_key: The name of the memory key in the prompt.\n",
    "        system_message: The system message to use. By default, a basic one will\n",
    "            be used.\n",
    "        verbose: Whether or not the final AgentExecutor should be verbose or not,\n",
    "            defaults to False.\n",
    "        max_token_limit: The max number of tokens to keep around in memory.\n",
    "            Defaults to 2000.\n",
    "\n",
    "    Returns:\n",
    "        An agent executor initialized appropriately\n",
    "    \"\"\"\n",
    "\n",
    "    if remember_intermediate_steps:\n",
    "        memory: BaseMemory = AgentTokenBufferMemory(\n",
    "            memory_key=memory_key, llm=llm, max_token_limit=max_token_limit\n",
    "        )\n",
    "    else:\n",
    "        memory = ConversationTokenBufferMemory(\n",
    "            memory_key=memory_key,\n",
    "            return_messages=True,\n",
    "            output_key=\"output\",\n",
    "            llm=llm,\n",
    "            max_token_limit=max_token_limit,\n",
    "        )\n",
    "\n",
    "    _system_message = system_message or _get_default_system_message()\n",
    "    \n",
    "    print(MessagesPlaceholder(variable_name=memory_key))\n",
    "    prompt = OpenAIFunctionsAgent.create_prompt(\n",
    "        system_message=_system_message,\n",
    "        extra_prompt_messages=[MessagesPlaceholder(variable_name=memory_key)],\n",
    "    )\n",
    "    print(\"prompt\", prompt)\n",
    "    agent = OpenAIFunctionsAgentDev(llm=llm, tools=tools, prompt=prompt)\n",
    "    return AgentExecutor(\n",
    "        agent=agent,\n",
    "        tools=tools,\n",
    "        memory=memory,\n",
    "        verbose=verbose,\n",
    "        return_intermediate_steps=remember_intermediate_steps,\n",
    "        **kwargs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "1d9483f0-3835-4337-ba75-04570a859160",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable_name='chat_history'\n",
      "prompt input_variables=['agent_scratchpad', 'chat_history', 'input'] input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]} messages=[SystemMessage(content='Do your best to answer the questions. Feel free to use any tools available to look up relevant information, only if necessary'), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')]\n",
      "variable_name='chat_history'\n",
      "prompt input_variables=['agent_scratchpad', 'chat_history', 'input'] input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]} messages=[SystemMessage(content='Do your best to answer the questions. Feel free to use any tools available to look up relevant information, only if necessary'), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')]\n"
     ]
    }
   ],
   "source": [
    "agent_executor_mistral = create_conversational_retrieval_agent_dev(mistral_llm, tools, verbose=True)\n",
    "agent_executor_openai = create_conversational_retrieval_agent_dev(llm, tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "1897f0bb-57a8-4bc7-a312-cfa2bc7c386e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.tools.render import format_tool_to_openai_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "63c4db43-84de-456e-bc88-f5662f562a50",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "BaseRunManager.__init__() missing 3 required keyword-only arguments: 'run_id', 'handlers', and 'inheritable_handlers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[277], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mCallbackManagerForChainRun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mget_child()\n",
      "\u001b[0;31mTypeError\u001b[0m: BaseRunManager.__init__() missing 3 required keyword-only arguments: 'run_id', 'handlers', and 'inheritable_handlers'"
     ]
    }
   ],
   "source": [
    "CallbackManagerForChainRun().get_child()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "01e54561-5fa4-44d1-9c7e-10ed16ac008f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "functions = [dict(format_tool_to_openai_function(t)) for t in tools]\n",
    "intermediate_steps = []\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content='Use external functions for real-time sports data queries.'),\n",
    "    HumanMessage(content='What\\'s the latest performance data for Gibbs in fantasy football? Should I start him this week?')\n",
    "]\n",
    "\n",
    "response = mistral_llm.predict_messages(\n",
    "    messages,\n",
    "    functions=functions,\n",
    "    callbacks=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "5fdd8987-bd8b-46f3-81c6-8ef198d5adb2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\\nAI: I can provide you with real-time performance data for Gibbs in fantasy football by querying an external API. However, I cannot predict whether or not you should start him this week as that decision is ultimately up to you based on your specific league rules and strategies.')"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "89420ea8-1cba-4185-9fca-0859f0d248ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-8YO6S0MQWZ0fWulCK8kLOdNcG9HUO', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\\n  \"query\": \"Gibbs fantasy football performance\"\\n}', name='football_search'), tool_calls=None))], created=1703206496, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=21, prompt_tokens=85, total_tokens=106))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"query\": \"Gibbs fantasy football performance\"\\n}', 'name': 'football_search'}})"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict_messages(\n",
    "                messages,\n",
    "                functions=functions,\n",
    "                callbacks=None,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "397f8dd6-42c3-4cba-aca7-d96dfd5115e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import (\n",
    "    AsyncCallbackManagerForChainRun,\n",
    "    AsyncCallbackManagerForToolRun,\n",
    "    CallbackManagerForChainRun,\n",
    "    CallbackManagerForToolRun,\n",
    "    Callbacks,\n",
    "    CallbackManager\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "00242e04-65f1-4518-902b-6dedb920b5c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "intermediate_steps []\n",
      "agent scratchpad []\n",
      "with functions True\n",
      "messages [SystemMessage(content='Do your best to answer the questions. Feel free to use any tools available to look up relevant information, only if necessary'), HumanMessage(content='should i start gibbs?')]\n",
      "format tools [{'name': 'football_search', 'description': 'Searches and returns documents regarding fantasy football.', 'parameters': {'title': 'RetrieverInput', 'type': 'object', 'properties': {'query': {'title': 'Query', 'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query']}}]\n",
      "functions [{'name': 'football_search', 'description': 'Searches and returns documents regarding fantasy football.', 'parameters': {'title': 'RetrieverInput', 'type': 'object', 'properties': {'query': {'title': 'Query', 'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query']}}]\n",
      "callbacks <langchain_core.callbacks.manager.CallbackManager object at 0x7fd0edace8c0>\n",
      "format tools [{'name': 'football_search', 'description': 'Searches and returns documents regarding fantasy football.', 'parameters': {'title': 'RetrieverInput', 'type': 'object', 'properties': {'query': {'title': 'Query', 'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query']}}]\n",
      "predicted_message <class 'langchain_core.messages.ai.AIMessage'> content='\\nAI: It depends on what you mean by \"Gibbs.\" If you\\'re asking whether you should start a new project or initiative, it\\'s important to consider your goals, resources, and potential impact. If you have a clear plan and the necessary resources, starting a new project can be a great way to grow and develop as an individual or organization. However, if you\\'re not sure about the feasibility of your idea or don\\'t have the necessary support, it may be better to wait until you have more information before making a decision.'\n",
      "predicted_message2  <class 'langchain_core.messages.ai.AIMessage'> content='' additional_kwargs={'function_call': {'arguments': '{\"query\": \"Gus Edwards\"}', 'name': 'football_search'}}\n",
      "agent_decision return_values={'output': '\\nAI: It depends on what you mean by \"Gibbs.\" If you\\'re asking whether you should start a new project or initiative, it\\'s important to consider your goals, resources, and potential impact. If you have a clear plan and the necessary resources, starting a new project can be a great way to grow and develop as an individual or organization. However, if you\\'re not sure about the feasibility of your idea or don\\'t have the necessary support, it may be better to wait until you have more information before making a decision.'} log='\\nAI: It depends on what you mean by \"Gibbs.\" If you\\'re asking whether you should start a new project or initiative, it\\'s important to consider your goals, resources, and potential impact. If you have a clear plan and the necessary resources, starting a new project can be a great way to grow and develop as an individual or organization. However, if you\\'re not sure about the feasibility of your idea or don\\'t have the necessary support, it may be better to wait until you have more information before making a decision.'\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "AI: It depends on what you mean by \"Gibbs.\" If you're asking whether you should start a new project or initiative, it's important to consider your goals, resources, and potential impact. If you have a clear plan and the necessary resources, starting a new project can be a great way to grow and develop as an individual or organization. However, if you're not sure about the feasibility of your idea or don't have the necessary support, it may be better to wait until you have more information before making a decision.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "result = agent_executor_mistral({\"input\": \"should i start gibbs?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "01a2c4cf-f542-4c8b-b8ef-df676378a7ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "intermediate_steps []\n",
      "agent scratchpad []\n",
      "with functions True\n",
      "messages [SystemMessage(content='Do your best to answer the questions. Feel free to use any tools available to look up relevant information, only if necessary'), HumanMessage(content='should I start trevor lawrence?'), AIMessage(content=\"As an AI, I don't have access to real-time information such as player performance or injury updates. However, I can provide some general advice on starting Trevor Lawrence.\\n\\nTrevor Lawrence is a rookie quarterback for the Jacksonville Jaguars. It's important to consider several factors when deciding whether to start him in fantasy football:\\n\\n1. Matchup: Evaluate the matchup for the week. Look at the opposing team's defense and how they have performed against quarterbacks. If the matchup is favorable, it could be a good opportunity to start Lawrence.\\n\\n2. Performance: Consider Lawrence's recent performance. Look at his statistics, completion percentage, touchdowns, interceptions, and rushing yards. If he has been performing well, it might be a good idea to start him.\\n\\n3. Team Support: Assess the overall support Lawrence has from his team. Consider the offensive line, wide receivers, and running game. If he has a strong supporting cast, it could increase his chances of having a successful fantasy outing.\\n\\n4. Alternatives: Compare Lawrence to other quarterbacks on your roster. If you have a more reliable and consistent option, it might be better to start them instead.\\n\\nUltimately, the decision to start Trevor Lawrence or any other player depends on your specific league settings, roster composition, and personal preferences. It's always a good idea to do some research, consider expert opinions, and trust your instincts when making start/sit decisions in fantasy football.\"), HumanMessage(content='should i start gibbs?')]\n",
      "format tools [{'name': 'football_search', 'description': 'Searches and returns documents regarding fantasy football.', 'parameters': {'title': 'RetrieverInput', 'type': 'object', 'properties': {'query': {'title': 'Query', 'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query']}}]\n",
      "functions [{'name': 'football_search', 'description': 'Searches and returns documents regarding fantasy football.', 'parameters': {'title': 'RetrieverInput', 'type': 'object', 'properties': {'query': {'title': 'Query', 'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query']}}]\n",
      "callbacks <langchain_core.callbacks.manager.CallbackManager object at 0x7fd0ed98ab30>\n",
      "format tools [{'name': 'football_search', 'description': 'Searches and returns documents regarding fantasy football.', 'parameters': {'title': 'RetrieverInput', 'type': 'object', 'properties': {'query': {'title': 'Query', 'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query']}}]\n",
      "predicted_message <class 'langchain_core.messages.ai.AIMessage'> content='' additional_kwargs={'function_call': {'arguments': '{\\n  \"query\": \"Gibbs fantasy football\"\\n}', 'name': 'football_search'}}\n",
      "predicted_message2  <class 'langchain_core.messages.ai.AIMessage'> content='' additional_kwargs={'function_call': {'arguments': '{\"query\": \"Gus Edwards\"}', 'name': 'football_search'}}\n",
      "agent_decision tool='football_search' tool_input={'query': 'Gibbs fantasy football'} log=\"\\nInvoking: `football_search` with `{'query': 'Gibbs fantasy football'}`\\n\\n\\n\" message_log=[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"query\": \"Gibbs fantasy football\"\\n}', 'name': 'football_search'}})]\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `football_search` with `{'query': 'Gibbs fantasy football'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[Document(page_content='7. Raheem Mostert\\n\\nRB — MIA\\n\\n  8. A.J. Brown\\n\\nWR — PHI\\n\\n  9. Bijan Robinson\\n\\nRB — ATL\\n\\n  10. Jahmyr Gibbs\\n\\nRB — DET\\n\\nView all Flex Rankings\\n\\nFantasy football\\n\\n# Who Should I Start\\n\\n__\\n\\n__\\n\\nSee Advice  __\\n\\n# Product Updates\\n\\n  1. The NBA PrizePicks Cheat Sheet Has Arrived!\\n\\nDec 01 — BettingPros\\n\\n1', metadata={'source': 'https://www.fantasypros.com/2023/11/rival-fantasy-nfl-week-10/'}), Document(page_content='7. Raheem Mostert\\n\\nRB — MIA\\n\\n  8. A.J. Brown\\n\\nWR — PHI\\n\\n  9. Bijan Robinson\\n\\nRB — ATL\\n\\n  10. Jahmyr Gibbs\\n\\nRB — DET\\n\\nView all Flex Rankings\\n\\nFantasy football\\n\\n# Who Should I Start\\n\\n__\\n\\n__\\n\\nSee Advice  __\\n\\n# Product Updates\\n\\n  1. The NBA PrizePicks Cheat Sheet Has Arrived!\\n\\nDec 01 — BettingPros\\n\\n1', metadata={'source': 'https://www.fantasypros.com/2023/11/5-stats-to-know-before-setting-your-fantasy-lineup-week-10/'}), Document(page_content='7. Raheem Mostert\\n\\nRB — MIA\\n\\n  8. A.J. Brown\\n\\nWR — PHI\\n\\n  9. Bijan Robinson\\n\\nRB — ATL\\n\\n  10. Jahmyr Gibbs\\n\\nRB — DET\\n\\nView all Flex Rankings\\n\\nFantasy football\\n\\n# Who Should I Start\\n\\n__\\n\\n__\\n\\nSee Advice  __\\n\\n# Product Updates\\n\\n  1. The NBA PrizePicks Cheat Sheet Has Arrived!\\n\\nDec 01 — BettingPros\\n\\n1', metadata={'source': 'https://www.fantasypros.com/2023/11/nfl-week-10-sleeper-picks-player-predictions-2023/'}), Document(page_content='7. Raheem Mostert\\n\\nRB — MIA\\n\\n  8. A.J. Brown\\n\\nWR — PHI\\n\\n  9. Bijan Robinson\\n\\nRB — ATL\\n\\n  10. Jahmyr Gibbs\\n\\nRB — DET\\n\\nView all Flex Rankings\\n\\nFantasy football\\n\\n# Who Should I Start\\n\\n__\\n\\n__\\n\\nSee Advice  __\\n\\n# Product Updates\\n\\n  1. The NBA PrizePicks Cheat Sheet Has Arrived!\\n\\nDec 01 — BettingPros\\n\\n1', metadata={'source': 'https://www.fantasypros.com/2023/11/nfl-dfs-week-10-stacking-advice-picks-2023-fantasy-football/'})]\u001b[0mintermediate_steps [(AgentActionMessageLog(tool='football_search', tool_input={'query': 'Gibbs fantasy football'}, log=\"\\nInvoking: `football_search` with `{'query': 'Gibbs fantasy football'}`\\n\\n\\n\", message_log=[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"query\": \"Gibbs fantasy football\"\\n}', 'name': 'football_search'}})]), [Document(page_content='7. Raheem Mostert\\n\\nRB — MIA\\n\\n  8. A.J. Brown\\n\\nWR — PHI\\n\\n  9. Bijan Robinson\\n\\nRB — ATL\\n\\n  10. Jahmyr Gibbs\\n\\nRB — DET\\n\\nView all Flex Rankings\\n\\nFantasy football\\n\\n# Who Should I Start\\n\\n__\\n\\n__\\n\\nSee Advice  __\\n\\n# Product Updates\\n\\n  1. The NBA PrizePicks Cheat Sheet Has Arrived!\\n\\nDec 01 — BettingPros\\n\\n1', metadata={'source': 'https://www.fantasypros.com/2023/11/rival-fantasy-nfl-week-10/'}), Document(page_content='7. Raheem Mostert\\n\\nRB — MIA\\n\\n  8. A.J. Brown\\n\\nWR — PHI\\n\\n  9. Bijan Robinson\\n\\nRB — ATL\\n\\n  10. Jahmyr Gibbs\\n\\nRB — DET\\n\\nView all Flex Rankings\\n\\nFantasy football\\n\\n# Who Should I Start\\n\\n__\\n\\n__\\n\\nSee Advice  __\\n\\n# Product Updates\\n\\n  1. The NBA PrizePicks Cheat Sheet Has Arrived!\\n\\nDec 01 — BettingPros\\n\\n1', metadata={'source': 'https://www.fantasypros.com/2023/11/5-stats-to-know-before-setting-your-fantasy-lineup-week-10/'}), Document(page_content='7. Raheem Mostert\\n\\nRB — MIA\\n\\n  8. A.J. Brown\\n\\nWR — PHI\\n\\n  9. Bijan Robinson\\n\\nRB — ATL\\n\\n  10. Jahmyr Gibbs\\n\\nRB — DET\\n\\nView all Flex Rankings\\n\\nFantasy football\\n\\n# Who Should I Start\\n\\n__\\n\\n__\\n\\nSee Advice  __\\n\\n# Product Updates\\n\\n  1. The NBA PrizePicks Cheat Sheet Has Arrived!\\n\\nDec 01 — BettingPros\\n\\n1', metadata={'source': 'https://www.fantasypros.com/2023/11/nfl-week-10-sleeper-picks-player-predictions-2023/'}), Document(page_content='7. Raheem Mostert\\n\\nRB — MIA\\n\\n  8. A.J. Brown\\n\\nWR — PHI\\n\\n  9. Bijan Robinson\\n\\nRB — ATL\\n\\n  10. Jahmyr Gibbs\\n\\nRB — DET\\n\\nView all Flex Rankings\\n\\nFantasy football\\n\\n# Who Should I Start\\n\\n__\\n\\n__\\n\\nSee Advice  __\\n\\n# Product Updates\\n\\n  1. The NBA PrizePicks Cheat Sheet Has Arrived!\\n\\nDec 01 — BettingPros\\n\\n1', metadata={'source': 'https://www.fantasypros.com/2023/11/nfl-dfs-week-10-stacking-advice-picks-2023-fantasy-football/'})])]\n",
      "agent scratchpad [AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"query\": \"Gibbs fantasy football\"\\n}', 'name': 'football_search'}}), FunctionMessage(content=\"[Document(page_content='7. Raheem Mostert\\\\n\\\\nRB — MIA\\\\n\\\\n  8. A.J. Brown\\\\n\\\\nWR — PHI\\\\n\\\\n  9. Bijan Robinson\\\\n\\\\nRB — ATL\\\\n\\\\n  10. Jahmyr Gibbs\\\\n\\\\nRB — DET\\\\n\\\\nView all Flex Rankings\\\\n\\\\nFantasy football\\\\n\\\\n# Who Should I Start\\\\n\\\\n__\\\\n\\\\n__\\\\n\\\\nSee Advice  __\\\\n\\\\n# Product Updates\\\\n\\\\n  1. The NBA PrizePicks Cheat Sheet Has Arrived!\\\\n\\\\nDec 01 — BettingPros\\\\n\\\\n1', metadata={'source': 'https://www.fantasypros.com/2023/11/rival-fantasy-nfl-week-10/'}), Document(page_content='7. Raheem Mostert\\\\n\\\\nRB — MIA\\\\n\\\\n  8. A.J. Brown\\\\n\\\\nWR — PHI\\\\n\\\\n  9. Bijan Robinson\\\\n\\\\nRB — ATL\\\\n\\\\n  10. Jahmyr Gibbs\\\\n\\\\nRB — DET\\\\n\\\\nView all Flex Rankings\\\\n\\\\nFantasy football\\\\n\\\\n# Who Should I Start\\\\n\\\\n__\\\\n\\\\n__\\\\n\\\\nSee Advice  __\\\\n\\\\n# Product Updates\\\\n\\\\n  1. The NBA PrizePicks Cheat Sheet Has Arrived!\\\\n\\\\nDec 01 — BettingPros\\\\n\\\\n1', metadata={'source': 'https://www.fantasypros.com/2023/11/5-stats-to-know-before-setting-your-fantasy-lineup-week-10/'}), Document(page_content='7. Raheem Mostert\\\\n\\\\nRB — MIA\\\\n\\\\n  8. A.J. Brown\\\\n\\\\nWR — PHI\\\\n\\\\n  9. Bijan Robinson\\\\n\\\\nRB — ATL\\\\n\\\\n  10. Jahmyr Gibbs\\\\n\\\\nRB — DET\\\\n\\\\nView all Flex Rankings\\\\n\\\\nFantasy football\\\\n\\\\n# Who Should I Start\\\\n\\\\n__\\\\n\\\\n__\\\\n\\\\nSee Advice  __\\\\n\\\\n# Product Updates\\\\n\\\\n  1. The NBA PrizePicks Cheat Sheet Has Arrived!\\\\n\\\\nDec 01 — BettingPros\\\\n\\\\n1', metadata={'source': 'https://www.fantasypros.com/2023/11/nfl-week-10-sleeper-picks-player-predictions-2023/'}), Document(page_content='7. Raheem Mostert\\\\n\\\\nRB — MIA\\\\n\\\\n  8. A.J. Brown\\\\n\\\\nWR — PHI\\\\n\\\\n  9. Bijan Robinson\\\\n\\\\nRB — ATL\\\\n\\\\n  10. Jahmyr Gibbs\\\\n\\\\nRB — DET\\\\n\\\\nView all Flex Rankings\\\\n\\\\nFantasy football\\\\n\\\\n# Who Should I Start\\\\n\\\\n__\\\\n\\\\n__\\\\n\\\\nSee Advice  __\\\\n\\\\n# Product Updates\\\\n\\\\n  1. The NBA PrizePicks Cheat Sheet Has Arrived!\\\\n\\\\nDec 01 — BettingPros\\\\n\\\\n1', metadata={'source': 'https://www.fantasypros.com/2023/11/nfl-dfs-week-10-stacking-advice-picks-2023-fantasy-football/'})]\", name='football_search')]\n",
      "with functions True\n",
      "messages [SystemMessage(content='Do your best to answer the questions. Feel free to use any tools available to look up relevant information, only if necessary'), HumanMessage(content='should I start trevor lawrence?'), AIMessage(content=\"As an AI, I don't have access to real-time information such as player performance or injury updates. However, I can provide some general advice on starting Trevor Lawrence.\\n\\nTrevor Lawrence is a rookie quarterback for the Jacksonville Jaguars. It's important to consider several factors when deciding whether to start him in fantasy football:\\n\\n1. Matchup: Evaluate the matchup for the week. Look at the opposing team's defense and how they have performed against quarterbacks. If the matchup is favorable, it could be a good opportunity to start Lawrence.\\n\\n2. Performance: Consider Lawrence's recent performance. Look at his statistics, completion percentage, touchdowns, interceptions, and rushing yards. If he has been performing well, it might be a good idea to start him.\\n\\n3. Team Support: Assess the overall support Lawrence has from his team. Consider the offensive line, wide receivers, and running game. If he has a strong supporting cast, it could increase his chances of having a successful fantasy outing.\\n\\n4. Alternatives: Compare Lawrence to other quarterbacks on your roster. If you have a more reliable and consistent option, it might be better to start them instead.\\n\\nUltimately, the decision to start Trevor Lawrence or any other player depends on your specific league settings, roster composition, and personal preferences. It's always a good idea to do some research, consider expert opinions, and trust your instincts when making start/sit decisions in fantasy football.\"), HumanMessage(content='should i start gibbs?'), AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"query\": \"Gibbs fantasy football\"\\n}', 'name': 'football_search'}}), FunctionMessage(content=\"[Document(page_content='7. Raheem Mostert\\\\n\\\\nRB — MIA\\\\n\\\\n  8. A.J. Brown\\\\n\\\\nWR — PHI\\\\n\\\\n  9. Bijan Robinson\\\\n\\\\nRB — ATL\\\\n\\\\n  10. Jahmyr Gibbs\\\\n\\\\nRB — DET\\\\n\\\\nView all Flex Rankings\\\\n\\\\nFantasy football\\\\n\\\\n# Who Should I Start\\\\n\\\\n__\\\\n\\\\n__\\\\n\\\\nSee Advice  __\\\\n\\\\n# Product Updates\\\\n\\\\n  1. The NBA PrizePicks Cheat Sheet Has Arrived!\\\\n\\\\nDec 01 — BettingPros\\\\n\\\\n1', metadata={'source': 'https://www.fantasypros.com/2023/11/rival-fantasy-nfl-week-10/'}), Document(page_content='7. Raheem Mostert\\\\n\\\\nRB — MIA\\\\n\\\\n  8. A.J. Brown\\\\n\\\\nWR — PHI\\\\n\\\\n  9. Bijan Robinson\\\\n\\\\nRB — ATL\\\\n\\\\n  10. Jahmyr Gibbs\\\\n\\\\nRB — DET\\\\n\\\\nView all Flex Rankings\\\\n\\\\nFantasy football\\\\n\\\\n# Who Should I Start\\\\n\\\\n__\\\\n\\\\n__\\\\n\\\\nSee Advice  __\\\\n\\\\n# Product Updates\\\\n\\\\n  1. The NBA PrizePicks Cheat Sheet Has Arrived!\\\\n\\\\nDec 01 — BettingPros\\\\n\\\\n1', metadata={'source': 'https://www.fantasypros.com/2023/11/5-stats-to-know-before-setting-your-fantasy-lineup-week-10/'}), Document(page_content='7. Raheem Mostert\\\\n\\\\nRB — MIA\\\\n\\\\n  8. A.J. Brown\\\\n\\\\nWR — PHI\\\\n\\\\n  9. Bijan Robinson\\\\n\\\\nRB — ATL\\\\n\\\\n  10. Jahmyr Gibbs\\\\n\\\\nRB — DET\\\\n\\\\nView all Flex Rankings\\\\n\\\\nFantasy football\\\\n\\\\n# Who Should I Start\\\\n\\\\n__\\\\n\\\\n__\\\\n\\\\nSee Advice  __\\\\n\\\\n# Product Updates\\\\n\\\\n  1. The NBA PrizePicks Cheat Sheet Has Arrived!\\\\n\\\\nDec 01 — BettingPros\\\\n\\\\n1', metadata={'source': 'https://www.fantasypros.com/2023/11/nfl-week-10-sleeper-picks-player-predictions-2023/'}), Document(page_content='7. Raheem Mostert\\\\n\\\\nRB — MIA\\\\n\\\\n  8. A.J. Brown\\\\n\\\\nWR — PHI\\\\n\\\\n  9. Bijan Robinson\\\\n\\\\nRB — ATL\\\\n\\\\n  10. Jahmyr Gibbs\\\\n\\\\nRB — DET\\\\n\\\\nView all Flex Rankings\\\\n\\\\nFantasy football\\\\n\\\\n# Who Should I Start\\\\n\\\\n__\\\\n\\\\n__\\\\n\\\\nSee Advice  __\\\\n\\\\n# Product Updates\\\\n\\\\n  1. The NBA PrizePicks Cheat Sheet Has Arrived!\\\\n\\\\nDec 01 — BettingPros\\\\n\\\\n1', metadata={'source': 'https://www.fantasypros.com/2023/11/nfl-dfs-week-10-stacking-advice-picks-2023-fantasy-football/'})]\", name='football_search')]\n",
      "format tools [{'name': 'football_search', 'description': 'Searches and returns documents regarding fantasy football.', 'parameters': {'title': 'RetrieverInput', 'type': 'object', 'properties': {'query': {'title': 'Query', 'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query']}}]\n",
      "functions [{'name': 'football_search', 'description': 'Searches and returns documents regarding fantasy football.', 'parameters': {'title': 'RetrieverInput', 'type': 'object', 'properties': {'query': {'title': 'Query', 'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query']}}]\n",
      "callbacks <langchain_core.callbacks.manager.CallbackManager object at 0x7fd0ed98ab30>\n",
      "format tools [{'name': 'football_search', 'description': 'Searches and returns documents regarding fantasy football.', 'parameters': {'title': 'RetrieverInput', 'type': 'object', 'properties': {'query': {'title': 'Query', 'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query']}}]\n",
      "predicted_message <class 'langchain_core.messages.ai.AIMessage'> content=\"I found some information about Jahmyr Gibbs in fantasy football rankings. According to the rankings, Jahmyr Gibbs is currently ranked as the 10th flex option. However, it's important to note that these rankings may vary depending on the source and the specific scoring system of your fantasy league.\\n\\nWhen deciding whether to start Gibbs, consider the following factors:\\n\\n1. Role and Usage: Assess Gibbs' role in his team's offense and how involved he is in the game plan. Look at his recent performance, touches, and involvement in the passing game.\\n\\n2. Matchup: Evaluate the matchup for the week. Consider the strength of the opposing defense against the run and how they have performed against running backs. A favorable matchup can increase the chances of a successful fantasy outing.\\n\\n3. Alternatives: Compare Gibbs to other running backs or flex options on your roster. Consider their matchups, recent performance, and role in their respective offenses. If you have a more reliable and consistent option, it might be better to start them instead.\\n\\nUltimately, the decision to start Jahmyr Gibbs or any other player depends on your specific league settings, roster composition, and personal preferences. It's always a good idea to do some research, consider expert opinions, and trust your instincts when making start/sit decisions in fantasy football.\"\n",
      "predicted_message2  <class 'langchain_core.messages.ai.AIMessage'> content='' additional_kwargs={'function_call': {'arguments': '{\"query\": \"Gus Edwards\"}', 'name': 'football_search'}}\n",
      "agent_decision return_values={'output': \"I found some information about Jahmyr Gibbs in fantasy football rankings. According to the rankings, Jahmyr Gibbs is currently ranked as the 10th flex option. However, it's important to note that these rankings may vary depending on the source and the specific scoring system of your fantasy league.\\n\\nWhen deciding whether to start Gibbs, consider the following factors:\\n\\n1. Role and Usage: Assess Gibbs' role in his team's offense and how involved he is in the game plan. Look at his recent performance, touches, and involvement in the passing game.\\n\\n2. Matchup: Evaluate the matchup for the week. Consider the strength of the opposing defense against the run and how they have performed against running backs. A favorable matchup can increase the chances of a successful fantasy outing.\\n\\n3. Alternatives: Compare Gibbs to other running backs or flex options on your roster. Consider their matchups, recent performance, and role in their respective offenses. If you have a more reliable and consistent option, it might be better to start them instead.\\n\\nUltimately, the decision to start Jahmyr Gibbs or any other player depends on your specific league settings, roster composition, and personal preferences. It's always a good idea to do some research, consider expert opinions, and trust your instincts when making start/sit decisions in fantasy football.\"} log=\"I found some information about Jahmyr Gibbs in fantasy football rankings. According to the rankings, Jahmyr Gibbs is currently ranked as the 10th flex option. However, it's important to note that these rankings may vary depending on the source and the specific scoring system of your fantasy league.\\n\\nWhen deciding whether to start Gibbs, consider the following factors:\\n\\n1. Role and Usage: Assess Gibbs' role in his team's offense and how involved he is in the game plan. Look at his recent performance, touches, and involvement in the passing game.\\n\\n2. Matchup: Evaluate the matchup for the week. Consider the strength of the opposing defense against the run and how they have performed against running backs. A favorable matchup can increase the chances of a successful fantasy outing.\\n\\n3. Alternatives: Compare Gibbs to other running backs or flex options on your roster. Consider their matchups, recent performance, and role in their respective offenses. If you have a more reliable and consistent option, it might be better to start them instead.\\n\\nUltimately, the decision to start Jahmyr Gibbs or any other player depends on your specific league settings, roster composition, and personal preferences. It's always a good idea to do some research, consider expert opinions, and trust your instincts when making start/sit decisions in fantasy football.\"\n",
      "\u001b[32;1m\u001b[1;3mI found some information about Jahmyr Gibbs in fantasy football rankings. According to the rankings, Jahmyr Gibbs is currently ranked as the 10th flex option. However, it's important to note that these rankings may vary depending on the source and the specific scoring system of your fantasy league.\n",
      "\n",
      "When deciding whether to start Gibbs, consider the following factors:\n",
      "\n",
      "1. Role and Usage: Assess Gibbs' role in his team's offense and how involved he is in the game plan. Look at his recent performance, touches, and involvement in the passing game.\n",
      "\n",
      "2. Matchup: Evaluate the matchup for the week. Consider the strength of the opposing defense against the run and how they have performed against running backs. A favorable matchup can increase the chances of a successful fantasy outing.\n",
      "\n",
      "3. Alternatives: Compare Gibbs to other running backs or flex options on your roster. Consider their matchups, recent performance, and role in their respective offenses. If you have a more reliable and consistent option, it might be better to start them instead.\n",
      "\n",
      "Ultimately, the decision to start Jahmyr Gibbs or any other player depends on your specific league settings, roster composition, and personal preferences. It's always a good idea to do some research, consider expert opinions, and trust your instincts when making start/sit decisions in fantasy football.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "result = agent_executor_openai({\"input\": \"should i start gibbs?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "a5a71c55-537d-4fe5-9ca2-8f37d6aeafba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /opt/conda/lib/python3.10/site-packages (0.5.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2023.10.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2023.11.17)\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1107b6-5bbb-4cb7-9c93-fff6ec6aeacd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-gpu.m114",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-gpu:m114"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
